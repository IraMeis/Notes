# Стохастический градиентный спуск
- Применение
``` python
opt = tf.optimizers.SGD(learning_rate=0.02)
opt.apply_gradients(zip([dk, db], [k, b]))
```
- Лучше всего рассчитывать данные на мини-батчи
##### Проблемы 
- Застревает в локальных минимумах функции
## Метод моментов (метод импульсов)
- Придаёт момент алгоритму и по инерции выталкивать функцию из локального минимума
- Применение
``` python
opt = tf.optimizers.SGD(momentum=0.5, learning_rate=0.02)
```
## Метод Нестерова
- Моменты заглядывают немного вперёд (во избежании спотыкания)
- Применение
``` python
opt = tf.optimizers.SGD(momentum=0.5, nesterov=True, learning_rate=0.02)
```
##### Проблемы
- Не связан с рассматриваемыми параметрами
- Учитывает только историю изменения градиента
# Адаград
- Рассматривает не только градиент, но и параметры
- Чем ближе параметр к минимуму, тем меньше будет изменение
- Ускоряет рассчёт при разреженных данных
- Применение
``` python
opt = tf.optimizers.Adagrad(learning_rate=0.01)
```
##### Проблемы
- Постоянное уменьшение шага обучения
- Сложно подобрать шаг обучения
# Ададельта
- Сохранение части истории градиента
- Изменяет шаг обучения
- Логика совпадает с Адаград
- Применение
``` python
opt = tf.optimizers.Adadelta(learning_rate=1.0)
```
# RMSProp
- Не хранит историю градиента
- Применение
```python
opt = tf.optimizers.RMSprop(learning_rate=0.001)
```
# Adam
- Зарекомендовал как функцию поиска минимума потерь
- Часто применяется при обучении НС
- Применение
```python
opt = tf.optimizers.Adam(learning_rate=0.1)
```
